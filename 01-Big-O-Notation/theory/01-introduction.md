# Big O Notation: Introduction

**Navigation:** [üè† Home](../../README.md) > [üìö Big O Notation](../README.md) > [üìñ Theory](./00-index.md) > Introduction

## What is Big O Notation?

Big O Notation is a mathematical notation used to describe the limiting behavior of a function when the input size grows towards infinity. In computer science, we use it to classify algorithms based on their time and space complexity.

The "O" stands for "Order of," which refers to the growth rate of an algorithm's runtime or memory usage as the input size increases.

## Formal Definition

For functions f(n) and g(n), we say that f(n) = O(g(n)) if there exist positive constants c and n‚ÇÄ such that:

f(n) ‚â§ c √ó g(n) for all n ‚â• n‚ÇÄ

This means that f(n) grows at most as fast as g(n) for sufficiently large input sizes.

## Simplification Rules

When calculating Big O complexity, we follow these simplification rules:

1. **Drop Constants**: O(2n) ‚Üí O(n)
2. **Drop Lower-Order Terms**: O(n¬≤ + n) ‚Üí O(n¬≤)
3. **Focus on Dominant Terms**: In nested loops, the innermost loop dominates

---

**Navigation**
- [‚¨ÖÔ∏è Previous: Theory Index](./00-index.md)
- [‚¨ÜÔ∏è Up to Big O Overview](../README.md)
- [‚û°Ô∏è Next: Complexity Classes](./02-complexity-classes.md)